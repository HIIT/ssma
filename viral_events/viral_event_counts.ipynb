{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for computing by-date and total counts for the number of distinct authors, sources (e.g. Twitter, Facebook, or blog), and domains in the data. Also, gini coefficients are computed for each of these fields both on a by-date basis and for full data. Finally, the scripts generate lists of distinct sources and domains in the data for each date, lists links found in data text content, and counts post volumes per distinct source for each date.\n",
    "\n",
    "The data from which the counts and lists are generated is loaded using queries in the \"viral_event_queries.csv\" file, which is produced in the notebook \"viral_event_data_download\".\n",
    "\n",
    "This notebook produces two .csv files. The counts and lists for sources, domains and links are written to the output file \"counts_ginis_links.csv\", and the post counts per source are written to the output file \"post_counts_per_source.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hybra Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hybra import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "core.set_data_path(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list of dates and write as column headers to output .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "d1 = date(2014, 1, 1)  # start date\n",
    "d2 = date(2017, 5, 22)  # end date\n",
    "\n",
    "delta = d2 - d1        # timedelta\n",
    "\n",
    "# Create dates\n",
    "dates = []\n",
    "for i in range(delta.days + 1):\n",
    "    dates.append(d1 + timedelta(days=i))\n",
    "\n",
    "# Format dates as strings\n",
    "dates_str = []\n",
    "for date in dates:\n",
    "    dates_str.append(str(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "out = open('data/csv/counts_ginis_links.csv', 'wb') # Create .csv file for counts, ginis, and links\n",
    "out_post_per_source = open('data/csv/post_counts_per_source.csv', 'wb')  # Create .csv file for post counts per source\n",
    "\n",
    "writer = csv.writer( out, delimiter=',' )\n",
    "writer_posts_per_source = csv.writer( out_post_per_source, delimiter=',' )\n",
    "\n",
    "# Write header rows\n",
    "writer.writerow(['query / type'] + dates_str + ['all documents'])\n",
    "writer_posts_per_source.writerow(['query'] + dates_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for calculating counts from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def post_count(data):\n",
    "\n",
    "    # Count post volume by date and return as Counter object\n",
    "    \n",
    "    dates_ok = filter( lambda d: d['timestamp'] > datetime(1970,1,1,0,10), data )\n",
    "    dates = map( lambda d: d['timestamp'].date(), dates_ok )\n",
    "\n",
    "    return Counter( dates )\n",
    "\n",
    "\n",
    "def get_authors_by_date( data ):\n",
    "    \n",
    "    # Creates a dictionary with dates as keys and lists of authors as values, and a list of all authors in data.\n",
    "    \n",
    "    authors_by_date = {}\n",
    "    authors_total = []\n",
    "    \n",
    "    for d in data:\n",
    "        key = d['timestamp'].date()\n",
    "\n",
    "        # Add dates in data as keys to authors by date dictionary\n",
    "        if key not in authors_by_date: \n",
    "            authors_by_date[key] = [d['creator']]\n",
    "        else:\n",
    "            authors_by_date[key].append(d['creator'])\n",
    "\n",
    "        authors_total.append(d['creator'])\n",
    "    \n",
    "    return authors_by_date, authors_total\n",
    "\n",
    "def get_sources_by_date( data ):\n",
    "    \n",
    "    # Creates a dictionary with dates as keys and lists of sources as values, and a list of all sources in data.\n",
    "    \n",
    "    sources_by_date = {}\n",
    "    sources_total = []\n",
    "    \n",
    "    for d in data:\n",
    "        key = d['timestamp'].date()\n",
    "\n",
    "        s = d['source_detail']\n",
    "\n",
    "        # Format sources found in data\n",
    "        if 'twitter' in s:\n",
    "            s = 'twitter'\n",
    "        elif 'facebook' in s:\n",
    "            s = 'facebook'\n",
    "        elif 'blog' in s:\n",
    "            s = 'blog'\n",
    "        elif 'youtube' in s:\n",
    "            s = 'youtube'\n",
    "        elif 'instagram' in s:\n",
    "            s = 'instagram'\n",
    "        elif 'googleplus' in s:\n",
    "            s = 'googleplus'\n",
    "    \n",
    "        # Add dates in data as keys to sources by date dictionary\n",
    "        if key not in sources_by_date:\n",
    "            sources_by_date[key] = [s]\n",
    "        else:\n",
    "            sources_by_date[key].append(s)\n",
    "\n",
    "        sources_total.append(s)\n",
    "    \n",
    "    return sources_by_date, sources_total\n",
    "\n",
    "def get_domains_by_date( data ):\n",
    "    \n",
    "    # Creates a dictionary with dates as keys and lists of domains as values, and a list of all domains in data.\n",
    "    \n",
    "    domains_by_date = {}\n",
    "    domains_total = []\n",
    "    \n",
    "    for d in data:\n",
    "\n",
    "        key = d['timestamp'].date()\n",
    "\n",
    "        # Get domain from data url field\n",
    "        domain = '{uri.netloc}'.format( uri= urlparse( d['url'] ) ).replace('www.', '')\n",
    "\n",
    "        # If no domain found, infer from data source\n",
    "        if not domain:\n",
    "\n",
    "            if 'twitter' in d['source_detail']:\n",
    "                domain = 'twitter.com'\n",
    "            elif 'facebook' in d['source_detail']:\n",
    "                domain = 'facebook.com'\n",
    "            elif 'instagram' in d['source_detail']:\n",
    "                domain = 'instagram.com'\n",
    "            elif 'googleplus' in d['source_detail']:\n",
    "                domain = 'plus.google.com'\n",
    "            elif 'youtube' in d['source_detail']:\n",
    "                domain = 'youtube.com'\n",
    "        \n",
    "        # Add dates in data as keys to domains by date dictionary\n",
    "        if key not in domains_by_date:\n",
    "            domains_by_date[key] = [domain]\n",
    "        else:\n",
    "            domains_by_date[key].append(domain)\n",
    "\n",
    "        domains_total.append(domain)\n",
    "    \n",
    "    return domains_by_date, domains_total\n",
    "\n",
    "def author_count( data ):\n",
    "    \n",
    "    # Count distinct authors by date and the total number of distinct authors in the data.\n",
    "    \n",
    "    authors_by_date, authors_total = get_authors_by_date( data )\n",
    "\n",
    "    authors_count = {}\n",
    "\n",
    "    for key, value in authors_by_date.items():\n",
    "        authors_count[key] = len( set(authors_by_date[key]) ) # Get count of distinc authors for each date\n",
    "\n",
    "    return authors_count, len(set(authors_total)) # Return counts by date and the total count of distinct authors\n",
    "\n",
    "def source_count( data ):\n",
    "    \n",
    "    # Count distinct sources by date and the total number of distinct sources in data.\n",
    "    # Also lists distinct sources by date.\n",
    "\n",
    "    sources_by_date, sources_total = get_sources_by_date( data )\n",
    "    source_lists = {}\n",
    "        \n",
    "    sources_count = {}\n",
    "\n",
    "    for key, value in sources_by_date.items():\n",
    "        \n",
    "        # Get count of distinct sources for each date\n",
    "        sources_count[key] = len( set(sources_by_date[key]) )\n",
    "        \n",
    "        # Get list of distinct sources for each date as unicode string\n",
    "        sources_string = unicode(set(sources_by_date[key]))\n",
    "        \n",
    "        # Format the sources string\n",
    "        sources_string = sources_string.replace('set([', '').replace('])', '').replace(\"u'\", \"'\").replace(\"'\", \"\").replace('\"', '').replace(' ', '')\n",
    "        \n",
    "        source_lists[key] = sources_string\n",
    "    \n",
    "    # Return counts by date, total count of distinct sources, and lists of distinct sources by date\n",
    "    return sources_count, len(set(sources_total)), source_lists\n",
    "\n",
    "\n",
    "def domain_count( data ):\n",
    "    \n",
    "    # Count distinct domains by date and the total number of distinct domains in data.\n",
    "    # Also lists distinct domains by date.\n",
    "\n",
    "    domains_by_date, domains_total = get_domains_by_date( data )\n",
    "    domain_lists = {}\n",
    "        \n",
    "    domains_count = {} \n",
    "\n",
    "    for key, value in domains_by_date.items():\n",
    "        \n",
    "        # Get count of distinct domains for each date\n",
    "        domains_count[key] = len( set(domains_by_date[key]) )\n",
    "        \n",
    "        # Get list of distinct domains for each date as unicode string\n",
    "        domains_string = unicode(set(domains_by_date[key])) \n",
    "        \n",
    "        # Format the domains string\n",
    "        domains_string = domains_string.replace('set([', '').replace('])', '').replace(\"u'\", \"'\").replace('\"', '').replace(\"'\", \"\").replace('\"', '').replace(' ', '')\n",
    "        \n",
    "        domain_lists[key] = domains_string\n",
    "\n",
    "    # Return counts by date, total count of distinct domains, and lists of distinct domains by date\n",
    "    return domains_count, len(set(domains_total)), domain_lists\n",
    "\n",
    "\n",
    "def count_posts_per_source( data, ids ):\n",
    "    \n",
    "    # This method counts unique posts by source for each date and returns them as dictionary with dates as keys\n",
    "    # and post counts per source as values.\n",
    "\n",
    "    sources = {}\n",
    "    post_count_per_source = {}\n",
    "\n",
    "    for d in data:\n",
    "        \n",
    "        if d['_id'] not in ids: # Check whether post has been counted yet.\n",
    "        \n",
    "            ids.add(d['_id'])\n",
    "        \n",
    "            key = d['timestamp'].date()\n",
    "\n",
    "            s = d['source_detail']\n",
    "\n",
    "            if 'twitter' in s:\n",
    "                s = 'twitter'\n",
    "            elif 'facebook' in s:\n",
    "                s = 'facebook'\n",
    "            elif 'blog' in s:\n",
    "                s = 'blog'\n",
    "            elif 'youtube' in s:\n",
    "                s = 'youtube'\n",
    "            elif 'instagram' in s:\n",
    "                s = 'instagram'\n",
    "            elif 'googleplus' in s:\n",
    "                s = 'googleplus'\n",
    "\n",
    "            # Add dates in data as keys and sources as values\n",
    "            if key not in sources:\n",
    "                sources[key] = [s]\n",
    "            else:\n",
    "                sources[key].append(s)\n",
    "    \n",
    "    # Count number of distinct sources by date and format as unicode strings.\n",
    "    for key, value in sources.items():\n",
    "        post_count_per_source[key] = unicode(Counter(sources[key])).replace('Counter', '').replace('({', '').replace('})', '').replace(\"'\", '')\n",
    "        \n",
    "    return post_count_per_source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for computing gini coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from urlparse import urlparse\n",
    "\n",
    "def compute_gini(counts):\n",
    "    \n",
    "    # Computes gini coefficient for given counts of values in data\n",
    "\n",
    "    n = len(counts)\n",
    "\n",
    "    if n == 0:\n",
    "        return None\n",
    "    elif n == 1:\n",
    "        return 1.0\n",
    "\n",
    "    counts = counts.values()\n",
    "    counts = sorted(counts)\n",
    "\n",
    "    up = 0\n",
    "    down = 0\n",
    "\n",
    "    for i in range(0, n):\n",
    "\n",
    "        up += ((2 * (i + 1)) - n - 1) * counts[i]\n",
    "        down += counts[i]\n",
    "\n",
    "    down *= n\n",
    "\n",
    "    gini = float(up) / float(down)\n",
    "\n",
    "    return gini\n",
    "\n",
    "\n",
    "def compute_gini_for(field, data):\n",
    "    \n",
    "    # Compute gini coefficients for given field by date and all values of given field in data\n",
    "    \n",
    "    if field == 'authors':\n",
    "        values_by_date, values_all = get_authors_by_date( data )\n",
    "    elif field == 'sources':\n",
    "        values_by_date, values_all = get_sources_by_date( data )\n",
    "    else:\n",
    "        values_by_date, values_all = get_domains_by_date( data )\n",
    "\n",
    "    gini_by_date = {}\n",
    "\n",
    "    for key, value in values_by_date.items():\n",
    "        gini_by_date[key] = compute_gini(collections.Counter(value))\n",
    "        \n",
    "    gini_all = compute_gini(collections.Counter(values_all))\n",
    "\n",
    "    return gini_by_date, gini_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method for extracting links found in data text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_links(data):\n",
    "    \n",
    "    # Get links by date in data and return as dictionary with dates as keys and list of links as values.\n",
    "    \n",
    "    link_lists = {}\n",
    "    \n",
    "    for d in data:\n",
    "        key = d['timestamp'].date()\n",
    "        \n",
    "        if 'links' not in d.keys():\n",
    "            continue\n",
    "                \n",
    "        if key not in link_lists:\n",
    "            link_lists[key] = [d['links']]\n",
    "        else:\n",
    "            link_lists[key].append(d['links'])\n",
    "            \n",
    "    return link_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for writing counts to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The methods for writing post, author, source, and domain counts all work by getting the counts by date\n",
    "# and writing these and total counts to the .csv out file for counts, ginis and links. \n",
    "# Counts by date are written to their corresponding columns, whereas total counts are written to the end of each row.\n",
    "\n",
    "def write_post_count(data, q, writer):\n",
    "    row = [q + ' / post counts']\n",
    "\n",
    "    total = 0\n",
    "    counts = post_count(data)\n",
    "\n",
    "    for date in dates:\n",
    "        if date in counts.keys():\n",
    "            row = row + [counts[date]]\n",
    "            total += int(counts[date])\n",
    "        else:\n",
    "            row = row + [0]\n",
    "\n",
    "    writer.writerow(row + [total])\n",
    "\n",
    "def write_author_count(data, q, writer):\n",
    "    row = [q + ' / author counts']\n",
    "\n",
    "    counts, total = author_count(data)\n",
    "\n",
    "    for date in dates:\n",
    "        if date in counts.keys():\n",
    "            row = row + [counts[date]]\n",
    "        else:\n",
    "            row = row + [0]\n",
    "\n",
    "    writer.writerow(row + [total])\n",
    "\n",
    "def write_source_count(data, q, writer):\n",
    "    row = [q + ' / source counts']\n",
    "\n",
    "    counts, total, source_lists = source_count(data)\n",
    "\n",
    "    for date in dates:\n",
    "        if date in counts.keys():\n",
    "            row = row + [counts[date]]\n",
    "        else:\n",
    "            row = row + [0]\n",
    " \n",
    "    writer.writerow(row + [total])\n",
    "    \n",
    "    row = [q + ' / source list']\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in source_lists.keys():\n",
    "            row = row + [source_lists[date]]\n",
    "        else:\n",
    "            row = row + [' ']\n",
    "    \n",
    "    writer.writerow(row)\n",
    "        \n",
    "\n",
    "def write_domain_count(data, q, writer):\n",
    "    row = [q + ' / domain counts']\n",
    "    \n",
    "    counts, total, domain_lists = domain_count(data)\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in counts.keys():\n",
    "            row = row + [counts[date]]\n",
    "        else:\n",
    "            row = row + [0]\n",
    "    \n",
    "    writer.writerow(row + [total])\n",
    "    \n",
    "    row = [q + ' / domain list']\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in domain_lists.keys():\n",
    "            row = row + [domain_lists[date]]\n",
    "        else:\n",
    "            row = row + [' ']\n",
    "    \n",
    "    writer.writerow(row)\n",
    "\n",
    "\n",
    "# This method gets the gini coefficients by date and for all data for the given field in data and writes\n",
    "# the to the .csv out file for counts, ginis and links. Ginis by date are written to their corresponding columns \n",
    "# and total ginis are written to the end of each row.\n",
    "    \n",
    "def write_gini( field, data, q, writer ):\n",
    "    row = [q + ' / ' + field +' gini']\n",
    "    \n",
    "    gini, gini_all = compute_gini_for( field, data )\n",
    "    \n",
    "    for date in dates:\n",
    "        if date in gini.keys():\n",
    "            row = row + [gini[date]]\n",
    "        else:\n",
    "            row = row + [0]\n",
    "        \n",
    "    writer.writerow(row + [gini_all])\n",
    "\n",
    "    \n",
    "# This method gets link lists by date and writes them as unicode strings to their corresponding columns in the\n",
    "# .csv out file for counts, ginis and links.\n",
    "\n",
    "def write_links(data, q, writer):\n",
    "    row = [q + ' / links']\n",
    "    \n",
    "    # Get links from data\n",
    "    links = extract_links(data)\n",
    "    \n",
    "    for date in dates: # Only write links within the specified date range\n",
    "        if date in links.keys():\n",
    "            \n",
    "            # Format links\n",
    "            links_string = unicode(links[date])\n",
    "            links_string = links_string.replace('[],', '').replace(\"[u'\", '')\n",
    "            links_string = links_string.replace(\"']\", '').replace('[', '')\n",
    "            links_string = links_string.replace(']', '').replace(\"u'\", \"\").replace(\"'\", \"\")\n",
    "            \n",
    "            row = row + [links_string]\n",
    "        else:\n",
    "            row = row + [' ']\n",
    "    \n",
    "    writer.writerow(row)  \n",
    "\n",
    "\n",
    "# This method gets post counts per source by date and writes them as unicode strings to their corresponding columns\n",
    "# in the .csv out file for post counts per source.\n",
    "    \n",
    "def write_post_count_per_source(data, q, writer, ids):\n",
    "    \n",
    "    post_count_per_source = count_posts_per_source(data, ids)\n",
    "    \n",
    "    if not post_count_per_source: # If dictionary empty ,just return\n",
    "        del post_count_per_source\n",
    "        return\n",
    "    \n",
    "    row = [q]    \n",
    "        \n",
    "    for date in dates: # Match post counts per source with their corresponding dates\n",
    "        if date in post_count_per_source.keys():\n",
    "            row = row + [post_count_per_source[date]]\n",
    "        else:\n",
    "            row = row + ['0']\n",
    "    \n",
    "    writer.writerow(row)\n",
    "    \n",
    "    del post_count_per_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get queries from filenames in data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a number of the queries are case-sensitive. If your filesystem is case-insensitive and you have downloaded the case-sensitive subset of queries into a separate directory, modify the below scripts to get queries from the separate directory and write them in the .csv file separately.\n",
    "\n",
    "The following queries come in both lowercase and uppercase varieties:\n",
    "\n",
    "text.hashtag:HIFKlive  \n",
    "text.hashtag:hifklive  \n",
    "text.hashtag:Huoneentaulu  \n",
    "text.hashtag:huoneentaulu  \n",
    "text.hashtag:IsacElliotFollowSpree  \n",
    "text.hashtag:isacelliotfollowspree  \n",
    "text.hashtag:kakutus  \n",
    "text.hashtag:kaKUtus  \n",
    "text.hashtag:KOVAA  \n",
    "text.hashtag:Kovaa  \n",
    "text.hashtag:MiskalleKoti  \n",
    "text.hashtag:miskallekoti  \n",
    "text.hashtag:Museokortti  \n",
    "text.hashtag:museokortti  \n",
    "text.hashtag:SDPlive  \n",
    "text.hashtag:sdplive  \n",
    "text.hashtag:SJS2014  \n",
    "text.hashtag:sjs2014  \n",
    "text.hashtag:Taiteeniltakoulu  \n",
    "text.hashtag:taiteeniltakoulu  \n",
    "text.hashtag:työTetris  \n",
    "text.hashtag:työtetris  \n",
    "text.hashtag:Vero150v  \n",
    "text.hashtag:vero150v  \n",
    "text.hashtag:VIIMEISENKERRAN  \n",
    "text.hashtag:ViimeisenKerran  \n",
    "text.hashtag:Visio2025  \n",
    "text.hashtag:visio2025  \n",
    "text.hashtag:WU19  \n",
    "text.hashtag:wu19  \n",
    "text.hashtag:TongueOutTuesday  \n",
    "text.hashtag:tongueouttuesday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries = []\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "path += \"/data/json/\"\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    if f[0] != '.':\n",
    "        queries.append( f.replace('.json', '') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open data and write counts, ginis, links, and post counts per source to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = set() # For checking duplicate posts in write_post_counts_per_source method \n",
    "\n",
    "for q in queries:\n",
    "    print 'Loading data: ' + q + '...'\n",
    "    data = list( core.data('futusome', folder = 'json/', query = q) )\n",
    "\n",
    "    print 'Writing...'\n",
    "    \n",
    "    write_post_count(data, q, writer)\n",
    "    write_author_count(data, q, writer)\n",
    "    write_source_count(data, q, writer)\n",
    "    write_domain_count(data, q, writer)\n",
    "    write_gini('authors', data, q, writer)\n",
    "    write_gini('sources', data, q, writer)\n",
    "    write_gini('domains', data, q, writer)\n",
    "    write_links(data, q, writer)\n",
    "    write_post_count_per_source(data, q, writer_posts_per_source, ids)\n",
    "    \n",
    "    del data # Delete to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.close()\n",
    "out_post_per_source.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
