{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for generating query strings from Futusome viral event data, contained in file \"keyword_hashtags_initials.csv\", and for downloading the data using the generated strings.\n",
    "\n",
    "The \"keyword_hashtags_initials.csv\" file contained columns for event id, Futusome score, event type, various other quantities describing the event, and the query string corresponding to the event.\n",
    "\n",
    "This notebook produces the file \"viral_event_queries.csv\", which contains the queries to be used in data download, and the file \"queries_orig_match.csv\", which contains the queries mapped to event ids and origin times in Futusome data.\n",
    "\n",
    "Event origin times are used for getting data before and after the event origin in the notebook \"event_select_days.ipynb\". \n",
    "\n",
    "The scripts generate a .csv file as an output in each step. These can be used for inspecting how the query strings are being processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hybra Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hybra import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "core.set_data_path(\"viral_events/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for generating queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_queries(path, out_path):\n",
    "    \n",
    "    # Get query strings and save to .csv file given in out_path\n",
    "\n",
    "    with open(path, 'rb') as f:\n",
    "\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "\n",
    "        reader.next() # Skip file headings\n",
    "\n",
    "        out = open(out_path, 'wb') # Create .csv file for query strings\n",
    "\n",
    "        writer = csv.writer(out, delimiter=',')\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            writer.writerow([row[25]]) # Write query string to .csv\n",
    "\n",
    "        out.close()\n",
    "\n",
    "        \n",
    "def format_queries(query_path, out_path):\n",
    "    \n",
    "    # Format query strings not to contain platform types part and save to .csv file given in out_path\n",
    "\n",
    "    with open(query_path, 'rb') as f:\n",
    "\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "\n",
    "        out = open(out_path, \"wb\") # Create .csv file for formatted queries\n",
    "\n",
    "        writer = csv.writer(out, delimiter=',')\n",
    "        \n",
    "        for row in reader: # Format queries not to contain platform types\n",
    "\n",
    "            query = row[0]\n",
    "            query = query.replace('type:twitter_tweet AND ', '')\n",
    "            query = query.replace('type:facebook* AND ', '')\n",
    "            query = query.replace('type:instagram* AND ', '')\n",
    "            query = query.replace(' AND type:facebook*', '')\n",
    "            \n",
    "            writer.writerow( [query] ) # Write formatted query to file\n",
    "\n",
    "        out.close()\n",
    "        \n",
    "               \n",
    "def remove_duplicate_queries(path, out_path):\n",
    "    \n",
    "    # Remove duplicates from pruned queries and save to .csv file given in out_path \n",
    "\n",
    "    reader = csv.reader(open(path, 'rb'), delimiter=',')\n",
    "\n",
    "    out = open(out_path, \"wb\") # Create .csv file for unique queries\n",
    "    \n",
    "    writer = csv.writer(out, delimiter=',')\n",
    "\n",
    "    dupl_removed = set()\n",
    "\n",
    "    for row in reader:\n",
    "        dupl_removed.add(row[0]) # Only keep unique queries\n",
    "\n",
    "    for q in dupl_removed:\n",
    "        writer.writerow( [q] ) # Write unique queries to file\n",
    "    \n",
    "    out.close()\n",
    "        \n",
    "        \n",
    "def match_query_ids(orig_file, queries_file, out_path):\n",
    "    \n",
    "    # Match queries to ids of the original events and write with original queries to .csv file given in out_path\n",
    "    # Note that formatted queries can match more than one original queries and thus more than one event id\n",
    "\n",
    "    reader_queries = csv.reader(open(queries_file, 'rb'), delimiter = ',')\n",
    "    \n",
    "    reader_orig = csv.reader(open(orig_file, 'rb'), delimiter = ',')\n",
    "\n",
    "    out = open(out_path, 'wb') # Create .csv for mapping queries to ids and original queries\n",
    "    \n",
    "    writer = csv.writer(out, delimiter = ',') \n",
    "\n",
    "    writer.writerow( ['query', 'event_id', 'orig_query'] ) # Create header row\n",
    "\n",
    "    reader_orig.next() # Skip header\n",
    "    \n",
    "    orig = []\n",
    "    for row in reader_orig:\n",
    "        orig.append([row[25], row[0]]) # Get original queries and corresponding ids\n",
    "\n",
    "    for query in reader_queries:\n",
    "\n",
    "        q = query[0]\n",
    "\n",
    "        for item in orig:\n",
    "            if q in item[0]:\n",
    "                writer.writerow( [q, item[1], item[0]] ) # Write queries and each match on own row in out file\n",
    "    \n",
    "    out.close()\n",
    "\n",
    "                \n",
    "def find_query_orig_dates(orig_file, queries_id_file, out_path):\n",
    "    \n",
    "    # Match queries to event origin times in Futusome viral events data and write to .csv file given in out_path\n",
    "    # Note that formatted queries can match more than one event id and thus have more than one origin time\n",
    "\n",
    "    reader_orig = csv.reader(open(orig_file, 'rb'), delimiter = ',')\n",
    "    \n",
    "    reader_id = csv.reader(open(queries_id_file, 'rb'), delimiter = ',')\n",
    "\n",
    "    out = open(out_path, 'wb') # Create .csv file for mapping queries to event origin times\n",
    "    \n",
    "    writer = csv.writer(out, delimiter = ',')\n",
    "\n",
    "    reader_id.next() # Skip headers\n",
    "\n",
    "    # Create a dictionary for matching formatted queries to event ids and origin times\n",
    "    match = {}\n",
    "    for row in reader_id:\n",
    "        \n",
    "        # Use formatted queries as keys and add dictionary for ids and origin times as value for each key\n",
    "        if row[0] not in match.keys(): \n",
    "            match[row[0]] = {'ids' : [row[1]], 'dates' : []}\n",
    "        else:\n",
    "            match[row[0]]['ids'].append(row[1])\n",
    "\n",
    "    # Get origin times from Futusome data\n",
    "    orig_data = {}\n",
    "    for row in reader_orig: # Use event id as key and origin time as value\n",
    "        orig_data[row[0]] = row[10]\n",
    "\n",
    "    for q in match.keys():\n",
    "        for i in match[q]['ids']:\n",
    "            # Match origin times from Futusome data to their corresponding queries using event ids\n",
    "            match[q]['dates'].append(orig_data[i])\n",
    "\n",
    "    writer.writerow(['query', 'id', 'orig_at']) # Write header row\n",
    "    \n",
    "    for key, value in match.items():\n",
    "        \n",
    "        # Format event ids and origin times and write with corresponding queries to out file\n",
    "        \n",
    "        ids = str(value['ids']).replace('[', '')\n",
    "        ids = ids.replace('\\'', '')\n",
    "        ids = ids.replace(']', '')\n",
    "        ids = ids.replace(', ', ';')\n",
    "\n",
    "        dates = str(value['dates']).replace('[', '')\n",
    "        dates = dates.replace('\\'', '')\n",
    "        dates = dates.replace(']', '')\n",
    "        dates = dates.replace(', ', ';')\n",
    "\n",
    "        writer.writerow([key, ids, dates])\n",
    "    \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate queries and origin times and save in .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_queries('data/csv/keywords_hashtags_initial.csv', 'data/csv/viral_event_queries.csv')\n",
    "format_queries('data/csv/viral_event_queries.csv', 'data/csv/queries_formatted_dupl.csv')\n",
    "remove_duplicate_queries('data/csv/queries_formatted_dupl.csv', 'data/csv/queries_formatted.csv')\n",
    "match_query_ids('data/csv/keywords_hashtags_initial.csv', 'data/csv/queries_formatted.csv', 'data/csv/queries_id_matched.csv')\n",
    "find_query_orig_dates('data/csv/viral_events.csv', 'data/csv/queries_id_matched.csv', 'data/csv/queries_orig_matched.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read queries from .csv and download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are a number of queries which are case-sensitive. If your file system is case-insensitive, you should download these queries into a separate directory to avoid overwriting downloaded data.\n",
    "\n",
    "The following queries come in both lowercase and uppercase varieties:\n",
    "\n",
    "text.hashtag/HIFKlive  \n",
    "text.hashtag/Huoneentaulu  \n",
    "text.hashtag/isacelliotfollowspree  \n",
    "text.hashtag/kakutus  \n",
    "text.hashtag/KOVAA  \n",
    "text.hashtag/miskallekoti  \n",
    "text.hashtag/museokortti  \n",
    "text.hashtag/SDPlive  \n",
    "text.hashtag/SJS2014  \n",
    "text.hashtag/taiteeniltakoulu \n",
    "text.hashtag/työTetris  \n",
    "text.hashtag/vero150v  \n",
    "text.hashtag/ViimeisenKerran  \n",
    "text.hashtag/Visio2025  \n",
    "text.hashtag/wu19  \n",
    "text.hashtag/TongueOutTuesday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "queries = []\n",
    "\n",
    "# Get queries from file and add to list\n",
    "with open('data/csv/viral_event_queries.csv', 'rb') as f:\n",
    "\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "\n",
    "    for row in reader:\n",
    "        queries.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data download requires a Futusome API key\n",
    "for q in queries:\n",
    "    data = list( core.data('futusome', folder = 'json/', query = q , api_key = '') )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
