{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes as inputs two files, called 'counts_ginis.csv' and 'queries_origin_matched.csv' and produces as output a file called 'event_summary.csv' which is used in the actual analysis.\n",
    "\n",
    "First, the script reads data about the query-events from counts_ginis.csv. Then, it uses data from queries_origin_matched.csv to filter the data so that only a certain number of days before and after the event's time of origin are considered.\n",
    "\n",
    "Note that there is no 1-to-1 mapping between Futusome events and queries, so a query-event may have multiple origin dates. Due to this, the script checks if two or more origin times that correspond to a query are included within the time window mentioned above. If so, the queries are considered to form a single event. Otherwise, they are treated as different events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we're interested in looking at some number of days before and after an event's time of origin. This part can be used to set up these parameters; the paper used days_before = 0 and days_before = 30. Also set up the final date to be used, which here is 2017-05-17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days_before = 0\n",
    "days_after = 30\n",
    "interval = datetime.timedelta(days = days_before + days_after)\n",
    "#interval = datetime.timedelta(days = 30)\n",
    "final_date = datetime.date(2017, 5, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event queries contain some incorrect characters, this is used to correct them. Note that the characters here are not the regular 'ö', 'ä', 'Ö' and 'Ä' although they look like them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrong_ae = 'ä'\n",
    "wrong_oe = 'ö'\n",
    "wrong_OE = 'Ö'\n",
    "wrong_AE = '̈A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and combine events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read queries and corresponding data. This script also replaces the faulty characters mentioned above with '*'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_numbers(path):\n",
    "\n",
    "    ndict = collections.defaultdict(dict)\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "\n",
    "        reader = csv.DictReader(f, delimiter = ',')\n",
    "\n",
    "        for row in reader:\n",
    "\n",
    "            ## The one row for each query and each value type\n",
    "            ## Let's split the index and fix broken letters\n",
    "            #query_count = row['query / count'].replace(wrong_ae, 'ä').replace(wrong_AE, 'Ä').replace(wrong_oe, 'ö').replace(wrong_OE, 'Ö')\n",
    "\n",
    "            ## This is 'query / type' for 15062017 onwards,\n",
    "            ## 'query / count' before that\n",
    "            query_count = row['query / type'].replace(wrong_ae, '*').replace(wrong_AE, '*').replace(wrong_oe, '*').replace(wrong_OE, '*')\n",
    "            query_count = query_count.split(' / ')\n",
    "            query = query_count[0]\n",
    "            value_type = query_count[1]\n",
    "            row.pop('query / type')\n",
    "            ndict[query][value_type] = row\n",
    "            \n",
    "    return ndict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function checks if two or more origin times that map to a query are included within the same time window and, if so, combines them. Used as a helper function by select_days()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_events(orig_ats, event_ids, interval):\n",
    "    \n",
    "    i = 0\n",
    "    j = 1\n",
    "\n",
    "    clean_origins = set()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ## Here we'll also add an id to each event\n",
    "\n",
    "        if j >= len(orig_ats):\n",
    "            if len(orig_ats) == 1:\n",
    "                clean_origins.add((orig_ats[0], event_ids[0]))\n",
    "            break\n",
    "\n",
    "        first = orig_ats[i]\n",
    "        first_id = event_ids[i]\n",
    "        second = orig_ats[j]\n",
    "        second_id = event_ids[j]\n",
    "\n",
    "        if second - first <= interval:\n",
    "            clean_origins.add((first, first_id))\n",
    "            j += 1\n",
    "        else:\n",
    "            clean_origins.add((first, first_id))\n",
    "            clean_origins.add((second, second_id))\n",
    "            i += 1\n",
    "            j += 1\n",
    "            \n",
    "    return clean_origins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops through the data associated with a query. Discards a query if some of the data is missing. Otherwise takes each part of the event data (i.e. information about sources, authors etc. per day) as a dict, appends the events origin time, event id, corresponding query and data type and returns a list containing these dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_query_data(query_data, orig_at, event_id, _query):\n",
    "\n",
    "    origs_to_add = []\n",
    "    \n",
    "    for k,v in query_data.iteritems():\n",
    "        \n",
    "        ## If every type of count (posts count, domains count)\n",
    "        ## has something other than zero in the first slot, \n",
    "        ## the event will be added to the list. If it has a zero,\n",
    "        ## something's broken and the event will be discarded\n",
    "        ## and its query printed.\n",
    "\n",
    "        ## The current data set has three broken events, I believe.\n",
    "\n",
    "        v = v.copy()\n",
    "\n",
    "        if v[str(orig_at)] == str(0):\n",
    "            print 'Error at: ' + _query\n",
    "            return False, origs_to_add\n",
    "\n",
    "        v['orig_at'] = orig_at\n",
    "        v['event_id'] = event_id\n",
    "        v['query'] = _query\n",
    "        v['value_type'] = k\n",
    "\n",
    "        origs_to_add.append(v)\n",
    "        \n",
    "    return True, origs_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function selects data from days falling within the time interval specified above from an event's origin. It also replaces the faulty characters mentioned earlier with '*'s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_days(path, numdict):\n",
    "\n",
    "    events = []\n",
    "\n",
    "    ## How many days before and after origin at are looked at        \n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "\n",
    "        reader = csv.DictReader(f, delimiter = ',')\n",
    "\n",
    "        for row in reader:\n",
    "            query = row['query']\n",
    "            query = row['query'].replace('ö', '*').replace('ä', '*').replace('Ö', '*').replace('Ä', '*')\n",
    "            \n",
    "            ## Here, there may be multiple origin dates and event ids\n",
    "            ## for a given query, so let's separate them\n",
    "            event_ids = row['id'].split(';')\n",
    "            orig_ats = row['orig_at'].split(';')\n",
    "            orig_dates = []\n",
    "\n",
    "            ## Consider each origin_at date. If the temporal overlap\n",
    "            ## between two origin_ats related to an event is large enough,\n",
    "            ## treat it as multiple independent events.\n",
    "\n",
    "            for i in range(0, len(orig_ats)):\n",
    "                ## Clean out milliseconds\n",
    "                orig_at = orig_ats[i].split('.')[0]\n",
    "                orig_at = datetime.datetime.strptime(orig_at, '%Y-%m-%d %H:%M:%S').date()\n",
    "                orig_ats[i] = orig_at\n",
    "\n",
    "            clean_origins = combine_events(orig_ats, event_ids, interval)\n",
    "\n",
    "            ## Loop through each event 'version'\n",
    "            version = 0\n",
    "\n",
    "            for origin in clean_origins:\n",
    "\n",
    "                ## This part makes sure that only events that\n",
    "                ## do not have incomplete data associated with them\n",
    "                ## are considered\n",
    "                \n",
    "                ## Consider each event and each id\n",
    "                orig_at = origin[0]\n",
    "                event_id = origin[1]\n",
    "\n",
    "                ## Fetch related data\n",
    "                query_data = numdict[query]\n",
    "                ## In case of multiple events correspond to a single query,\n",
    "                ## append 'version number' to the query name.\n",
    "                _query = query + '_' + str(version)\n",
    "\n",
    "                ## Go through the data associated with the query.\n",
    "                ## If loop_query_data() returns True, there were no\n",
    "                ## problems with the data so it's added to events.\n",
    "                success, origs_to_add = loop_query_data(query_data, orig_at, event_id, _query)\n",
    "\n",
    "                if success:\n",
    "                    for orig in origs_to_add:\n",
    "                        events.append(orig)\n",
    "\n",
    "                version += 1\n",
    "                \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = read_numbers('data/csv/counts_ginis_2017-08-27.csv')\n",
    "e = select_days('data/csv/queries_orig_matched_2017-08-24.csv', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn data into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(e)\n",
    "df = df.set_index(['query', 'value_type'])\n",
    "df.drop('all documents', axis = 1, inplace = True)\n",
    "df.drop('event_id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selected_days(columns):\n",
    "    orig_at = columns.loc['orig_at']\n",
    "    dates = columns.index\n",
    "    dates = dates.drop('orig_at')\n",
    "    \n",
    "    selected = []\n",
    "    \n",
    "    if orig_at + datetime.timedelta(days = days_after) > final_date:\n",
    "        return\n",
    "    \n",
    "    for date in dates:\n",
    "        column_date = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if column_date >= orig_at - datetime.timedelta(days = days_before) \\\n",
    "        and column_date < orig_at + datetime.timedelta(days = days_after):\n",
    "            if 'counts' in columns.name[1]:\n",
    "                selected.append(columns.loc[date])\n",
    "                    \n",
    "    return pd.Series(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.iloc[df.index.get_level_values('value_type').str.contains('counts')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.apply(selected_days, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form event summary file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how many days an event lasted, i.e. how many days it took for post count to drop to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_event_duration(columns):\n",
    "    \n",
    "    if (columns == '0').any() == False:\n",
    "        return 30\n",
    "    return int((columns == '0').argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the total of dataframe values during the time an event lasted, and the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_during_duration(columns):\n",
    "    \n",
    "    duration = columns['duration']\n",
    "    \n",
    "    active_days = columns[0:duration]\n",
    "    \n",
    "    return sum(active_days.astype(float))\n",
    "\n",
    "def average_during_duration(columns):\n",
    "    \n",
    "    duration = columns['duration']\n",
    "    \n",
    "    active_days = columns[0:duration]\n",
    "    \n",
    "    return sum(active_days.astype(float)) / duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually separate the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts_df = df.loc[(df.index.get_level_values('value_type') == 'post counts')]\n",
    "posts_df = posts_df.reset_index().drop('value_type', axis = 1).set_index('query')\n",
    "\n",
    "authors_df = df.loc[(df.index.get_level_values('value_type') == 'author counts')]\n",
    "authors_df = authors_df.reset_index().drop('value_type', axis = 1).set_index('query')\n",
    "\n",
    "domains_df = df.loc[(df.index.get_level_values('value_type') == 'domain counts')]\n",
    "domains_df = domains_df.reset_index().drop('value_type', axis = 1).set_index('query')\n",
    "\n",
    "sources_df = df.loc[(df.index.get_level_values('value_type') == 'source counts')]\n",
    "sources_df = sources_df.reset_index().drop('value_type', axis = 1).set_index('query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the dataframe functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duration_df = posts_df.apply(get_event_duration, axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Rename some columns\n",
    "\n",
    "posts_df.columns = [str(i) + ' posts' for i in range(0,30)]\n",
    "authors_df.columns = [str(i) + ' authors' for i in range(0,30)]\n",
    "domains_df.columns = [str(i) + ' domains' for i in range(0,30)]\n",
    "sources_df.columns = [str(i) + ' sources' for i in range(0,30)]\n",
    "\n",
    "## Add duration info to each data frame\n",
    "\n",
    "posts_df['duration'] = duration_df\n",
    "authors_df['duration'] = duration_df\n",
    "domains_df['duration'] = duration_df\n",
    "sources_df['duration'] = duration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Compute the total and average values of the variables\n",
    "## during the time the event was 'active'\n",
    "\n",
    "posts_df['total posts'] = posts_df.apply(total_during_duration, axis = 1)\n",
    "posts_df['average posts'] = posts_df.apply(average_during_duration, axis = 1)\n",
    "\n",
    "authors_df['total authors'] = authors_df.apply(total_during_duration, axis = 1)\n",
    "authors_df['average authors'] = authors_df.apply(average_during_duration, axis = 1)\n",
    "\n",
    "domains_df['total domains'] = domains_df.apply(total_during_duration, axis = 1)\n",
    "domains_df['average domains'] = domains_df.apply(average_during_duration, axis = 1)\n",
    "\n",
    "sources_df['total sources'] = sources_df.apply(total_during_duration, axis = 1)\n",
    "sources_df['average sources'] = sources_df.apply(average_during_duration, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recombine data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Combine the data into a single data frame\n",
    "\n",
    "combine_df = posts_df[['0 posts', '1 posts', '2 posts', 'total posts', 'average posts']]\n",
    "combine_df = pd.concat([combine_df, authors_df[['0 authors', '1 authors', '2 authors', 'total authors', 'average authors']]], axis = 1)\n",
    "combine_df = pd.concat([combine_df, domains_df[['0 domains', '1 domains', '2 domains', 'total domains', 'average domains']]], axis = 1)\n",
    "combine_df = pd.concat([combine_df, sources_df[['0 sources', '1 sources', '2 sources', 'total sources', 'average sources']]], axis = 1)\n",
    "combine_df = pd.concat([combine_df, posts_df[['duration']]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn query name into event name and event type\n",
    "\n",
    "Also removes things like 'AND NOT text.exact' as well\n",
    "as the marker for different 'iterations' of the same event,\n",
    "e.g. 'ykk*saamu_0' and 'ykk*saamu_1' both become 'ykk*saamu'\n",
    "\n",
    "Possible event types are hashtag and substantive (i.e. keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_event_name_and_type(columns):\n",
    "    query = columns['query']\n",
    "    \n",
    "    split = query.split(':')\n",
    "    event_type = split[0].split('.')[1]\n",
    "    event_name = split[1]\n",
    "    event_name = event_name.split(' ')[0]\n",
    "    event_name = event_name.rsplit('_')[0]\n",
    "    return pd.Series({'event name': event_name, 'event type': event_type})\n",
    "\n",
    "combine_df = combine_df.reset_index() \n",
    "combine_df[['event name', 'event type']] = combine_df.reset_index().apply(split_event_name_and_type, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three blocks wrangle the data frame, show it for inspection, and write it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_df = combine_df.drop('query', axis = 1)\n",
    "event_names = combine_df['event name']\n",
    "event_types = combine_df['event type']\n",
    "combine_df = combine_df.drop(['event name', 'event type'], axis = 1)\n",
    "combine_df.insert(0, 'event type', event_types)\n",
    "combine_df.insert(0, 'event name', event_names)\n",
    "combine_df = combine_df.set_index('event name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## And output it. Remember to set the proper file name!\n",
    "\n",
    "combine_df.to_csv('data/csv/event_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
